"""
pysteps.nowcasts.linda
======================

This module implements the Lagrangian INtegro-Difference equation model with
Autoregression (LINDA). The model combines extrapolation, S-PROG, STEPS, ANVIL,
integro-difference equation (IDE) and cell tracking methods. It can produce
both deterministic and probabilistic nowcasts. LINDA is particularly designed
for nowcasting intense localized rainfall. For this purpose, it is expected to
give better forecast skill than S-PROG or STEPS.

The model consists of the following components:

1. feature detection to identify rain cells
2. advection-based extrapolation
3. autoregressive integrated ARI(p,1) process for growth and decay
4. convolution to account for loss of predictability
5. stochastic perturbations to simulate forecast errors

To allow localization to cells containing intense rainfall, LINDA utilizes a
sparse feature-based representation of the input data. Building on extrapolation
nowcast, the temporal evolution of rainfall fields is modeled in the Lagrangian
coordinates. Using the ARI process is adapted from ANVIL :cite:`PCLH2020`, and
the convolution is adapted from the integro-difference equation (IDE)
methodology proposed in :cite:`FW2005` and :cite:`XWF2005`. The combination of
these two approaches essentially replaces the cascade-based autoregressive
process used in S-PROG and STEPS. Using the convolution gives several
advantages such as the ability to handle anisotropic structure, domain
boundaries and missing data. Based on the marginal distribution and covariance
structure of forecast errors, localized perturbations are generated by adapting
the short-space Fourier transform (SSFT) methodology developed in
:cite:`NBSG2017`.

.. autosummary::
    :toctree: ../generated/

    forecast
"""

import time

try:
    import dask

    DASK_IMPORTED = True
except ImportError:
    DASK_IMPORTED = False
import numpy as np
from scipy.integrate import nquad
from scipy.interpolate import interp1d
from scipy.optimize import least_squares, LinearConstraint, minimize, minimize_scalar
from scipy.signal import convolve
from scipy import stats
from pysteps import extrapolation
from pysteps import feature


def forecast(
    precip_fields,
    advection_field,
    timesteps,
    feature_method="blob",
    feature_kwargs={},
    ari_order=1,
    kernel_type="anisotropic",
    interp_window_radius=None,
    convol_window_radius=None,
    ari_window_radius=None,
    errdist_window_radius=None,
    acf_window_radius=None,
    extrap_method="semilagrangian",
    extrap_kwargs={},
    add_perturbations=True,
    num_ens_members=24,
    seed=None,
    num_workers=1,
    measure_time=False,
):
    """Generate a nowcast ensemble by using the Lagrangian INtegro-Difference
    equation model with Autoregression (LINDA).

    Parameters
    ----------
    precip_fields : array_like
        Array of shape (ari_order + 2, m, n) containing the input rain rate
        or reflectivity fields (in linear scale) ordered by timestamp from
        oldest to newest. The time steps between the inputs are assumed to be
        regular.
    advection_field : array_like
        Array of shape (2, m, n) containing the x- and y-components of the
        advection field. The velocities are assumed to represent one time step
        between the inputs.
    timesteps : int or list of floats
        Number of time steps to forecast or a list of time steps for which the
        forecasts are computed (relative to the input time step). The elements
        of the list are required to be in ascending order.
    feature_method : {'blob', 'domain' 'grid', 'shitomasi'}
        Feature detection method:

        +-------------------+-----------------------------------------------------+
        |    Method name    |                  Description                        |
        +===================+=====================================================+
        |  blob             | Laplacian of Gaussian (LoG) blob detector           |
        |                   | implemented in scikit-image                         |
        +-------------------+-----------------------------------------------------+
        |  domain           | no feature detection, the model is applied over the |
        |                   | whole domain without localization                   |
        +-------------------+-----------------------------------------------------+
        |  grid             | no feature detection: the coordinates of the        |
        |                   | localization windows are aligned in a grid          |
        +-------------------+-----------------------------------------------------+
        |  shitomasi        | Shi-Tomasi corner detector implemented in OpenCV    |
        +-------------------+-----------------------------------------------------+

        Default : 'blob'
    feature_kwargs : dict, optional
        Keyword arguments that are passed as **kwargs for the feature detector.
        See :py:mod:`pysteps.feature.blob` and :py:mod:`pysteps.feature.shitomasi`.
    ari_order : {1, 2}
        The order of the ARI(p,1) model.
    kernel_type : {"anisotropic", "isotropic"}
        The type of the kernel. Default : 'anisotropic'.
    interp_window_radius : float
        The standard deviation of the Gaussian kernel for computing the
        interpolation weights. Default : 0.2 * min(m, n).
    convol_window_radius : float
        The standard deviation of the Gaussian window for estimating the
        convolution parameters. Default : 0.15 * min(m, n).
    ari_window_radius : float
        The standard deviation of the Gaussian window for estimating the
        ARI parameters. Default : 0.25 * min(m, n).
    errdist_window_radius : float
        The standard deviation of the Gaussian window for estimating the
        forecast error distribution. Default : 0.15 * min(m, n).
    acf_window_radius : float
        The standard deviation of the Gaussian window for estimating the
        forecast error ACF. Default : 0.25 * min(m, n).
    extrap_method : str, optional
        The extrapolation method to use. See the documentation of
        :py:mod:`pysteps.extrapolation.interface`.
    extrap_kwargs : dict, optional
        Optional dictionary containing keyword arguments for the extrapolation
        method. See the documentation of :py:mod:`pysteps.extrapolation`.
    add_perturbations : bool
        Set to False to disable perturbations and generate a single
        deterministic nowcast.
    num_ens_members : int
        The number of ensemble members to generate.
    seed : int
        Optional seed for the random generator.
    num_workers : int
        The number of workers to use for parallel computations. Applicable if
        dask is installed. When num_workers>1, it is advisable to disable
        OpenMP by setting the environment variable OMP_NUM_THREADS to 1. This
        avoids slowdown caused by too many simultaneous threads.
    measure_time: bool
        If set to True, measure, print and return the computation time.

    Returns
    -------
    out : numpy.ndarray
        A four-dimensional array of shape (num_ens_members, num_timesteps, m, n)
        containing a time series of forecast precipitation fields for each
        ensemble member. If add_perturbations is False, the first dimension is
        dropped. The time series starts from t0 + timestep, where timestep is
        taken from the input fields.
    """
    _check_inputs(precip_fields, advection_field, timesteps, ari_order)

    if interp_window_radius is None:
        interp_window_radius = 0.2 * np.min(precip_fields.shape[1:])
    if convol_window_radius is None:
        convol_window_radius = 0.15 * np.min(precip_fields.shape[1:])
    if ari_window_radius is None:
        ari_window_radius = 0.25 * np.min(precip_fields.shape[1:])
    if add_perturbations:
        if errdist_window_radius is None:
            errdist_window_radius = 0.15 * min(
                precip_fields.shape[1], precip_fields.shape[2]
            )
        if acf_window_radius is None:
            acf_window_radius = 0.25 * min(
                precip_fields.shape[1], precip_fields.shape[2]
            )

    print("Computing LINDA nowcast:")
    print("------------------------")
    print("")

    print("Inputs:")
    print("-------")
    print(
        "input dimensions: {}x{}".format(precip_fields.shape[1], precip_fields.shape[2])
    )
    print("")

    print("Methods:")
    print("--------")
    if add_perturbations:
        print("nowcast type:     ensemble")
    else:
        print("nowcast type:     deterministic")
    print("feature detector: {}".format(feature_method))
    print("extrapolator:     {}".format(extrap_method))
    print("")

    print("Parameters:")
    print("-----------")
    if isinstance(timesteps, int):
        print("number of time steps:      {}".format(timesteps))
    else:
        # TODO: implement fractional time steps
        raise NotImplementedError("fractional time steps not yet implemented")
        print("time steps:                {}".format(timesteps))
    print("ARI model order:           {}".format(ari_order))
    print("convol. window radius:     {}".format(convol_window_radius))
    print("ARI window radius:         {}".format(ari_window_radius))
    print("interp. window radius:     {}".format(interp_window_radius))
    if add_perturbations:
        print("error dist. window radius: {}".format(errdist_window_radius))
        print("error ACF window radius:   {}".format(acf_window_radius))
        print("ensemble size:             {}".format(num_ens_members))
    print("parallel workers:          {}".format(num_workers))

    fct_gen = _linda_init(
        precip_fields,
        advection_field,
        timesteps,
        feature_method,
        feature_kwargs,
        ari_order,
        kernel_type,
        interp_window_radius,
        convol_window_radius,
        ari_window_radius,
        extrap_method,
        extrap_kwargs,
        add_perturbations,
        num_ens_members,
        num_workers,
        measure_time,
    )
    if measure_time:
        fct_gen, precip_fields_lagr_diff, init_time = fct_gen
    else:
        fct_gen, precip_fields_lagr_diff = fct_gen
    if not add_perturbations:
        return _linda_forecast(
            precip_fields,
            precip_fields_lagr_diff[1:],
            timesteps,
            fct_gen,
            None,
            measure_time,
            True,
        )
    else:
        print("Estimating forecast errors... ", end="", flush=True)

        if measure_time:
            starttime = time.time()

        precip_fct_det = _linda_forecast(
            precip_fields[:-1],
            precip_fields_lagr_diff[:-1],
            1,
            fct_gen,
            None,
            False,
            False,
        )

        # compute multiplicative forecast errors
        err = precip_fct_det[-1] / precip_fields[-1]

        # mask small values
        mask = np.logical_or(
            np.logical_and(precip_fct_det[-1] >= 1.0, precip_fields[-1] >= 0.5),
            np.logical_and(precip_fct_det[-1] >= 0.5, precip_fields[-1] >= 1.0),
        )
        err[~mask] = np.nan

        if measure_time:
            print("{:.2f} seconds.".format(time.time() - starttime))
        else:
            print("done.")

        pert_gen = _init_perturbation_generator(
            err,
            fct_gen,
            errdist_window_radius,
            acf_window_radius,
            interp_window_radius,
            measure_time,
            num_workers,
        )

        def worker():
            return _linda_forecast(
                precip_fields,
                precip_fields_lagr_diff[1:],
                timesteps,
                fct_gen,
                pert_gen,
                False,
                False,
            )

        # TODO: implement printing
        # TODO: implement measurement of computation time
        precip_fct_ensemble = []

        if DASK_IMPORTED and num_workers > 1:
            res = []
            for i in range(num_ens_members):
                res.append(dask.delayed(worker)())
            precip_fct_ensemble = dask.compute(
                *res, num_workers=num_workers, scheduler="threads"
            )
        else:
            precip_fct_ensemble.append(worker())

        return np.stack(precip_fct_ensemble), 0.0


def _check_inputs(precip_fields, advection_field, timesteps, ari_order):
    if ari_order not in [1, 2]:
        raise ValueError(f"ari_order {ari_order} given, 1 or 2 required")
    if len(precip_fields.shape) != 3:
        raise ValueError("precip_fields must be a three-dimensional array")
    if precip_fields.shape[0] < ari_order + 2:
        raise ValueError("precip_fields.shape[0] < ari_order+2")
    if len(advection_field.shape) != 3:
        raise ValueError("advection_field must be a three-dimensional array")
    if precip_fields.shape[1:3] != advection_field.shape[1:3]:
        raise ValueError(
            "dimension mismatch between precip_fields and advection_field: precip_fields.shape={}, advection_field.shape={}".format(
                precip_fields.shape, advection_field.shape
            )
        )
    if isinstance(timesteps, list) and not sorted(timesteps) == timesteps:
        raise ValueError("timesteps is not in ascending order")


# Compute a localized convolution by applying a set of kernels with the given
# spatial weights. The weights are assumed to be normalized.
def _composite_convolution(field, kernels, weights):
    n = len(kernels)
    field_c = 0.0

    for i in range(n):
        field_c += weights[i] * _masked_convolution(field, kernels[i])

    return field_c


# Compute the inverse ACF mapping between two distributions.
def _compute_inverse_acf_mapping(target_dist, target_dist_params, n_intervals=20):
    phi = (
        lambda x1, x2, rho: 1.0
        / (2 * np.pi * np.sqrt(1 - rho ** 2))
        * np.exp(-(x1 ** 2 + x2 ** 2 - 2 * rho * x1 * x2) / (2 * (1 - rho ** 2)))
    )

    rho_1 = np.linspace(-0.999, 0.999, n_intervals)
    rho_2 = np.empty(len(rho_1))

    mu = target_dist.mean(*target_dist_params)
    sigma = target_dist.std(*target_dist_params)

    cdf_trans = lambda x: target_dist.ppf(stats.norm.cdf(x), *target_dist_params)
    int_range = (-6, 6)

    for i, rho_1_ in enumerate(rho_1):
        f = (
            lambda x1, x2: (cdf_trans(x1) - mu)
            * (cdf_trans(x2) - mu)
            * phi(x1, x2, rho_1_)
        )
        opts = {"epsabs": 1e-4, "epsrel": 1e-4, "limit": 1}
        rho_2[i] = nquad(f, (int_range, int_range), opts=opts)[0] / (sigma * sigma)

    return interp1d(np.hstack([-1.0, rho_2, 1.0]), np.hstack([-1.0, rho_1, 1.0]))


# Compute anisotropic Gaussian convolution kernel.
def _compute_kernel_anisotropic(params, cutoff=6.0):
    phi, sigma1, sigma2, alpha = params

    sigma1 = abs(sigma1)
    sigma2 = abs(sigma2)

    phi_r = phi / 180.0 * np.pi
    R_inv = np.array([[np.cos(phi_r), np.sin(phi_r)], [-np.sin(phi_r), np.cos(phi_r)]])

    bb_y1, bb_x1, bb_y2, bb_x2 = _compute_ellipse_bbox(phi, sigma1, sigma2, cutoff)

    x = np.arange(int(bb_x1), int(bb_x2) + 1).astype(float)
    if len(x) % 2 == 0:
        x = np.arange(int(bb_x1) - 1, int(bb_x2) + 1).astype(float)
    y = np.arange(int(bb_y1), int(bb_y2) + 1).astype(float)
    if len(y) % 2 == 0:
        y = np.arange(int(bb_y1) - 1, int(bb_y2) + 1).astype(float)

    X, Y = np.meshgrid(x, y)
    XY = np.vstack([X.flatten(), Y.flatten()])
    XY = np.dot(R_inv, XY)

    x2 = XY[0, :] * XY[0, :]
    y2 = XY[1, :] * XY[1, :]
    result = np.exp(-((x2 / sigma1 + y2 / sigma2) ** alpha))
    result /= np.sum(result)

    return np.reshape(result, X.shape)


def _compute_kernel_isotropic(sigma, cutoff=6.0):
    bb_y1, bb_x1, bb_y2, bb_x2 = (
        -sigma * cutoff,
        -sigma * cutoff,
        sigma * cutoff,
        sigma * cutoff,
    )

    x = np.arange(int(bb_x1), int(bb_x2) + 1).astype(float)
    if len(x) % 2 == 0:
        x = np.arange(int(bb_x1) - 1, int(bb_x2) + 1).astype(float)
    y = np.arange(int(bb_y1), int(bb_y2) + 1).astype(float)
    if len(y) % 2 == 0:
        y = np.arange(int(bb_y1) - 1, int(bb_y2) + 1).astype(float)

    X, Y = np.meshgrid(x / sigma, y / sigma)

    r2 = X * X + Y * Y
    result = np.exp(-0.5 * r2)

    return result / np.sum(result)


# Compute the bounding box of an ellipse.
def _compute_ellipse_bbox(phi, sigma1, sigma2, cutoff):
    r1 = cutoff * sigma1
    r2 = cutoff * sigma2
    phi_r = phi / 180.0 * np.pi

    if np.abs(phi_r - np.pi / 2) > 1e-6 and np.abs(phi_r - 3 * np.pi / 2) > 1e-6:
        alpha = np.arctan(-r2 * np.sin(phi_r) / (r1 * np.cos(phi_r)))
        w = r1 * np.cos(alpha) * np.cos(phi_r) - r2 * np.sin(alpha) * np.sin(phi_r)

        alpha = np.arctan(r2 * np.cos(phi_r) / (r1 * np.sin(phi_r)))
        h = r1 * np.cos(alpha) * np.sin(phi_r) + r2 * np.sin(alpha) * np.cos(phi_r)
    else:
        w = sigma2 * cutoff
        h = sigma1 * cutoff

    return -abs(h), -abs(w), abs(h), abs(w)


# Compute parametric ACF.
def _compute_parametric_acf(params, m, n, func):
    c, phi, sigma1, sigma2 = params

    sigma1 = abs(sigma1)
    sigma2 = abs(sigma2)

    phi_r = phi / 180.0 * np.pi
    rot_inv = np.array(
        [[np.cos(phi_r), np.sin(phi_r)], [-np.sin(phi_r), np.cos(phi_r)]]
    )

    if m % 2 != 0 or n % 2 != 0:
        raise ValueError("m and n must be even")

    x = np.fft.ifftshift(np.arange(-int(n / 2), int(n / 2)))
    y = np.fft.ifftshift(np.arange(-int(m / 2), int(m / 2)))

    grid_x, grid_y = np.meshgrid(x, y)
    grid_xy = np.vstack([grid_x.flatten(), grid_y.flatten()])
    grid_xy = np.dot(rot_inv, grid_xy)

    grid_xy[0, :] = grid_xy[0, :] / sigma1
    grid_xy[1, :] = grid_xy[1, :] / sigma2

    r2 = np.reshape(
        grid_xy[0, :] * grid_xy[0, :] + grid_xy[1, :] * grid_xy[1, :], grid_x.shape
    )
    if func == "gauss":
        result = np.exp(-0.5 * r2)
    else:
        r2 = np.sqrt(r2)
        result = np.exp(-r2)

    return c * result


# Compute interpolation weights.
def _compute_window_weights(coords, grid_height, grid_width, window_radius):
    coords = coords.astype(float).copy()
    num_features = coords.shape[0]

    coords[:, 0] /= grid_height
    coords[:, 1] /= grid_width

    window_radius_1 = window_radius / grid_height
    window_radius_2 = window_radius / grid_width

    grid_x = (np.arange(grid_width) + 0.5) / grid_width
    grid_y = (np.arange(grid_height) + 0.5) / grid_height

    grid_x, grid_y = np.meshgrid(grid_x, grid_y)

    w = np.empty((num_features, grid_x.shape[0], grid_x.shape[1]))

    if coords.shape[0] > 1:
        for i, c in enumerate(coords):
            dy = c[0] - grid_y
            dx = c[1] - grid_x

            w[i, :] = np.exp(
                -dy * dy / (2 * window_radius_1 ** 2)
                - dx * dx / (2 * window_radius_2 ** 2)
            )
    else:
        w[0, :] = np.ones((grid_height, grid_width))

    return w


# Compute sample ACF from FFT.
def _compute_sample_acf(field):
    # TODO: let user choose the FFT method
    field_fft = np.fft.rfft2((field - np.mean(field)) / np.std(field))
    fft_abs = np.abs(field_fft * np.conj(field_fft))

    return np.fft.irfft2(fft_abs, s=field.shape) / (field.shape[0] * field.shape[1])


# Constrained optimization of AR(1) parameters.
def _estimate_ar1_params(
    field_src, field_dst, estim_weights, interp_weights, num_workers=1
):
    def objf(p, *args):
        i = args[0]
        field_ar = p * field_src
        return np.nansum(estim_weights[i] * (field_dst - field_ar) ** 2.0)

    bounds = (-0.98, 0.98)

    def worker(i):
        return minimize_scalar(objf, method="bounded", bounds=bounds, args=(i,)).x

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(len(estim_weights)):
            res.append(dask.delayed(worker)(i))

        psi = dask.compute(*res, num_workers=num_workers, scheduler="threads")
    else:
        psi = []
        for i in range(len(estim_weights)):
            psi.append(worker(i))

    return [np.sum([psi[i] * interp_weights[i] for i in range(len(psi))], axis=0)]


# Constrained optimization of AR(2) parameters.
def _estimate_ar2_params(
    field_src, field_dst, estim_weights, interp_weights, num_workers=1
):
    def objf(p, *args):
        i = args[0]
        field_ar = p[0] * field_src[1] + p[1] * field_src[0]
        return np.nansum(estim_weights[i] * (field_dst - field_ar) ** 2.0)

    bounds = [(-1.98, 1.98), (-0.98, 0.98)]
    constraints = [
        LinearConstraint(
            np.array([(1, 1), (-1, 1)]),
            (-np.inf, -np.inf),
            (0.98, 0.98),
            keep_feasible=True,
        )
    ]

    def worker(i):
        return minimize(
            objf,
            (0.8, 0.0),
            method="trust-constr",
            bounds=bounds,
            constraints=constraints,
            args=(i,),
        ).x

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(len(estim_weights)):
            res.append(dask.delayed(worker)(i))

        psi = dask.compute(*res, num_workers=num_workers, scheduler="threads")
    else:
        psi = []
        for i in range(len(estim_weights)):
            psi.append(worker(i))

    psi_out = []
    for i in range(2):
        psi_out.append(
            np.sum([psi[j][i] * interp_weights[j] for j in range(len(psi))], axis=0)
        )

    return psi_out


# Estimation of convolution kernel.
def _estimate_convol_params(
    field_src,
    field_dst,
    weights,
    mask,
    kernel_type="anisotropic",
    kernel_params={},
    num_workers=1,
):
    masks = []
    for i in range(len(weights)):
        masks.append(np.logical_and(mask, weights[i] > 1e-3))

    def objf_aniso(p, *args):
        i = args[0]
        p = _get_anisotropic_kernel_params(p)
        kernel = _compute_kernel_anisotropic(p, **kernel_params)

        field_src_c = _masked_convolution(field_src, kernel)
        fval = np.sqrt(weights[i][masks[i]]) * (
            field_dst[masks[i]] - field_src_c[masks[i]]
        )

        return fval

    def objf_iso(p, *args):
        i = args[0]
        kernel = _compute_kernel_isotropic(p, **kernel_params)

        field_src_c = _masked_convolution(field_src, kernel)
        fval = np.sum(
            weights[i][masks[i]] * (field_dst[masks[i]] - field_src_c[masks[i]]) ** 2
        )

        return fval

    def worker(i):
        if kernel_type == "anisotropic":
            bounds = np.array([(-10.0, -10.0, 0.25, 0.1), (10.0, 10.0, 4.0, 5.0)])
            p_opt = least_squares(
                objf_aniso,
                np.array((1.0, 1.0, 1.0, 1.0)),
                bounds=bounds,
                method="trf",
                ftol=1e-6,
                xtol=1e-6,
                gtol=1e-6,
                args=(i,),
            )
            p_opt = _get_anisotropic_kernel_params(p_opt.x)

            return _compute_kernel_anisotropic(p_opt, **kernel_params)
        else:
            p_opt = minimize_scalar(
                objf_iso, bounds=[0.01, 10.0], method="bounded", args=(i,)
            )
            p_opt = p_opt.x

            return _compute_kernel_isotropic(p_opt, **kernel_params)

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(len(weights)):
            res.append(dask.delayed(worker)(i))
        kernels = dask.compute(*res, num_workers=num_workers, scheduler="threads")
    else:
        kernels = []
        for i in range(len(weights)):
            kernels.append(worker(i))

    return kernels


# Fit a parametric ACF to the given sample estimate.
def _fit_acf(acf, method="trf"):
    def objf(p, *args):
        p = _get_anisotropic_kernel_params(p)
        fitted_acf = _compute_parametric_acf(p, acf.shape[0], acf.shape[1], "exp")

        return (acf - fitted_acf).flatten()

    bounds = np.array([(0.01, -10.0, -10.0, 0.1), (10.0, 10.0, 10.0, 10.0)])
    p_opt = least_squares(
        objf,
        np.array((1.0, 1.0, 1.0, 1.0)),
        bounds=bounds,
        method="trf",
        ftol=1e-6,
        xtol=1e-6,
        gtol=1e-6,
    )

    return _compute_parametric_acf(
        _get_anisotropic_kernel_params(p_opt.x), acf.shape[0], acf.shape[1], "exp"
    )


# Fit a lognormal distribution by maximizing the log-likelihood function with
# the constraint that the mean value is one.
def _fit_dist(err, dist, wf, mask):
    f = lambda p: -np.sum(np.log(stats.lognorm.pdf(err[mask], p, -0.5 * p ** 2)))
    p_opt = minimize_scalar(f, bounds=(1e-3, 20.0), method="Bounded")

    return (p_opt.x, -0.5 * p_opt.x ** 2)


# Generate perturbations based on the estimated forecast error statistics.
# TODO: restrict the perturbation generation inside the radar mask
def _generate_perturbations(pert_gen, num_workers, seed):
    m, n = pert_gen["m"], pert_gen["n"]
    dist_param = pert_gen["dist_param"]
    std = pert_gen["std"]
    acf_fft_ampl = pert_gen["acf_fft_ampl"]
    weights = pert_gen["weights"]

    noise_field = stats.norm.rvs(size=(m, n), random_state=seed)
    noise_field_fft = np.fft.rfft2(noise_field)

    output_field = np.zeros((m, n))

    def worker(i):
        if std[i] > 0.0:
            filtered_noise = np.fft.irfft2(acf_fft_ampl[i] * noise_field_fft, s=(m, n))
            filtered_noise /= np.std(filtered_noise)
            filtered_noise = stats.lognorm.ppf(
                stats.norm.cdf(filtered_noise), *dist_param[i]
            )
        else:
            filtered_noise = np.ones(weights[i].shape)

        return weights[i] * filtered_noise

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(weights.shape[0]):
            res.append(dask.delayed(worker)(i))
        res = dask.compute(*res, num_workers=num_workers, scheduler="threads")
        for r in res:
            output_field += r
    else:
        for i in range(weights.shape[0]):
            output_field += worker(i)

    return output_field


# Get anisotropic convolution kernel parameters from the given parameter vector.
def _get_anisotropic_kernel_params(p):
    theta = np.arctan2(p[1], p[0])
    sigma1 = np.sqrt(p[0] * p[0] + p[1] * p[1])
    sigma2 = sigma1 * p[2]

    return theta, sigma1, sigma2, p[3]


def _init_perturbation_generator(
    fct_err,
    fct_gen,
    errdist_window_radius,
    acf_window_radius,
    interp_window_radius,
    measure_time,
    num_workers,
):
    pert_gen = {}
    pert_gen["m"] = fct_err.shape[0]
    pert_gen["n"] = fct_err.shape[1]

    feature_coords = fct_gen["feature_coords"]

    print("Estimating perturbation parameters... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    mask_finite = np.isfinite(fct_err)

    fct_err = fct_err.copy()
    fct_err[~mask_finite] = 1.0

    weights_dist = _compute_window_weights(
        feature_coords,
        fct_err.shape[0],
        fct_err.shape[1],
        errdist_window_radius,
    )

    acf_winfunc = _window_tukey if feature_coords.shape[0] > 1 else _window_uniform

    def worker(i):
        weights_acf = acf_winfunc(
            fct_err.shape[0],
            fct_err.shape[1],
            feature_coords[i, 0],
            feature_coords[i, 1],
            acf_window_radius,
            acf_window_radius,
        )

        mask = np.logical_and(mask_finite, weights_dist[i] > 0.1)
        if np.sum(mask) > 10 and np.sum(np.abs(fct_err[mask] - 1.0) >= 1e-3) > 10:
            distpar = _fit_dist(fct_err, stats.lognorm, weights_dist[i], mask)
            inv_acf_mapping = _compute_inverse_acf_mapping(stats.lognorm, distpar)
            mask_acf = weights_acf > 1e-4
            std = _weighted_std(fct_err[mask_acf], weights_dist[i][mask_acf])
            acf = inv_acf_mapping(
                _compute_sample_acf(weights_acf * (fct_err - 1.0) / std)
            )
            acf = _fit_acf(acf)
        else:
            distpar = None
            std = None
            acf = None

        return distpar, std, np.sqrt(np.abs(np.fft.rfft2(acf)))

    dist_params = []
    stds = []
    acf_fft_ampl = []

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(feature_coords.shape[0]):
            res.append(dask.delayed(worker)(i))
        res = dask.compute(*res, num_workers=num_workers, scheduler="multiprocessing")
        for r in res:
            dist_params.append(r[0])
            stds.append(r[1])
            acf_fft_ampl.append(r[2])
    else:
        for i in range(feature_coords.shape[0]):
            r = worker(i)
            dist_params.append(r[0])
            stds.append(r[1])
            acf_fft_ampl.append(r[2])

    pert_gen["dist_param"] = dist_params
    pert_gen["std"] = stds
    pert_gen["acf_fft_ampl"] = acf_fft_ampl

    weights = _compute_window_weights(
        feature_coords,
        fct_err.shape[0],
        fct_err.shape[1],
        interp_window_radius,
    )
    pert_gen["weights"] = weights / np.sum(weights, axis=0)

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    return pert_gen


# Iterate autoregressive process.
# TODO: use the method implemented in pysteps.timeseries.autoregression
def _iterate_ar_model(input_fields, psi):
    input_field_new = 0.0

    for i in range(len(psi)):
        input_field_new += psi[i] * input_fields[-(i + 1), :]

    return np.concatenate([input_fields[1:, :], input_field_new[np.newaxis, :]])


# Compute LINDA nowcast.
def _linda_forecast(
    precip_fields,
    precip_fields_lagr_diff,
    timesteps,
    fct_gen,
    pert_gen,
    measure_time,
    print_info,
):
    # compute the nowcast
    advection_field = fct_gen["advection_field"]
    ari_order = fct_gen["ari_order"]
    extrapolator = fct_gen["extrapolator"]
    extrap_kwargs = fct_gen["extrap_kwargs"].copy()
    interp_weights = fct_gen["interp_weights"]
    kernels_1 = fct_gen["kernels_1"]
    kernels_2 = fct_gen["kernels_2"]
    mask_adv = fct_gen["mask_adv"]
    psi = fct_gen["psi"]
    num_workers = fct_gen["num_workers"]

    precip_fct = precip_fields[-1].copy()

    displacement = None
    extrap_kwargs["return_displacement"] = True
    precip_fct_out = []

    for i in range(precip_fields_lagr_diff.shape[0]):
        for j in range(ari_order - i):
            precip_fields_lagr_diff[i] = _composite_convolution(
                precip_fields_lagr_diff[i],
                kernels_1,
                interp_weights,
            )

    # iterate each time step
    if measure_time:
        starttime_mainloop = time.time()

    # TODO: Implement using a list instead of a number of fixed timesteps
    for t in range(timesteps):
        if print_info:
            print(
                "Computing nowcast for time step %d... " % (t + 1),
                end="",
                flush=True,
            )

        if measure_time:
            starttime = time.time()

        precip_fields_lagr_diff = _iterate_ar_model(precip_fields_lagr_diff, psi)
        precip_fct += precip_fields_lagr_diff[-1]
        for i in range(precip_fields_lagr_diff.shape[0]):
            precip_fields_lagr_diff[i] = _composite_convolution(
                precip_fields_lagr_diff[i],
                kernels_1,
                interp_weights,
            )
        precip_fct = _composite_convolution(precip_fct, kernels_2, interp_weights)

        precip_fct_out_ = precip_fct.copy()
        precip_fct_out_[precip_fct_out_ < 0.0] = 0.0
        precip_fct_out_[~mask_adv] = np.nan

        # apply perturbations
        if pert_gen is not None:
            # TODO: implement supplying seed number
            pert_field = _generate_perturbations(pert_gen, num_workers, None)
            precip_fct_out_ *= pert_field

        # advect the forecast field for t time steps
        extrap_kwargs["displacement_prev"] = displacement
        precip_fct_out_, displacement = extrapolator(
            precip_fct_out_, advection_field, 1, **extrap_kwargs
        )
        precip_fct_out.append(precip_fct_out_[0])

        if print_info:
            if measure_time:
                print("{:.2f} seconds.".format(time.time() - starttime))
            else:
                print("done.")

    if measure_time:
        mainloop_time = time.time() - starttime_mainloop
        return np.stack(precip_fct_out), mainloop_time
    else:
        return np.stack(precip_fct_out)


# Initialize LINDA nowcast model.
def _linda_init(
    precip_fields,
    advection_field,
    timesteps,
    feature_method,
    feature_kwargs,
    ari_order,
    kernel_type,
    interp_window_radius,
    convol_window_radius,
    ari_window_radius,
    extrap_method,
    extrap_kwargs,
    add_perturbations,
    num_ens_members,
    num_workers,
    measure_time,
):
    fct_gen = {}
    fct_gen["advection_field"] = advection_field
    fct_gen["ari_order"] = ari_order
    fct_gen["add_perturbations"] = add_perturbations
    fct_gen["num_workers"] = num_workers
    fct_gen["measure_time"] = measure_time

    precip_fields = precip_fields[-(ari_order + 2) :]
    input_length = precip_fields.shape[0]

    starttime_init = time.time()

    extrapolator = extrapolation.get_method(extrap_method)
    extrap_kwargs = extrap_kwargs.copy()
    extrap_kwargs["allow_nonfinite_values"] = True
    fct_gen["extrapolator"] = extrapolator
    fct_gen["extrap_kwargs"] = extrap_kwargs

    # detect features from the most recent input field
    # TODO: implement the "grid" option
    if feature_method in {"blob", "shitomasi"}:
        precip_field_ = precip_fields[-1].copy()
        precip_field_[~np.isfinite(precip_field_)] = 0.0
        feature_detector = feature.get_method(feature_method)

        if measure_time:
            starttime = time.time()

        feature_coords = np.fliplr(
            feature_detector(precip_field_, **feature_kwargs)[:, :2]
        )

        feature_type = "blobs" if feature_method == "blob" else "corners"
        print("")
        print("Detecting features... ", end="", flush=True)
        if measure_time:
            print(
                "found {} {} in {:.2f} seconds.".format(
                    feature_coords.shape[0], feature_type, time.time() - starttime
                )
            )
        else:
            print("found {} {}.".format(feature_coords.shape[0], feature_type))
    elif feature_method == "domain":
        feature_coords = np.zeros((1, 2), dtype=int)
    else:
        raise NotImplementedError(
            "feature detector '%s' not implemented" % feature_method
        )
    fct_gen["feature_coords"] = feature_coords

    # compute interpolation weights
    interp_weights = _compute_window_weights(
        feature_coords,
        precip_fields.shape[1],
        precip_fields.shape[2],
        interp_window_radius,
    )
    interp_weights /= np.sum(interp_weights, axis=0)
    fct_gen["interp_weights"] = interp_weights

    # transform the input fields to the Lagrangian coordinates
    precip_fields_lagr = np.empty(precip_fields.shape)

    def worker(i):
        precip_fields_lagr[i, :] = extrapolator(
            precip_fields[i, :],
            advection_field,
            input_length - 1 - i,
            "min",
            **extrap_kwargs,
        )[-1]

    if DASK_IMPORTED and num_workers > 1:
        res = []

    print("Transforming to Lagrangian coordinates... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    for i in range(precip_fields.shape[0] - 1):
        if DASK_IMPORTED and num_workers > 1:
            res.append(dask.delayed(worker)(i))
        else:
            worker(i)

    if DASK_IMPORTED and num_workers > 1:
        dask.compute(*res, num_workers=min(num_workers, len(res)), scheduler="threads")
    precip_fields_lagr[-1] = precip_fields[-1]

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    # compute advection mask and set nan to pixels, where one or more of the
    # advected input fields has a nan value
    mask_adv = np.all(np.isfinite(precip_fields_lagr), axis=0)
    fct_gen["mask_adv"] = mask_adv
    for i in range(precip_fields_lagr.shape[0]):
        precip_fields_lagr[i, ~mask_adv] = np.nan

    # compute differenced input fields in the Lagrangian coordinates
    precip_fields_lagr_diff = np.diff(precip_fields_lagr, axis=0)

    # estimate parameters of the deterministic model (i.e. the convolution and
    # the ARI process)

    print("Estimating the first convolution kernel... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    # estimate convolution kernel for the differenced component
    convol_weights = _compute_window_weights(
        feature_coords,
        precip_fields.shape[1],
        precip_fields.shape[2],
        convol_window_radius,
    )

    kernels_1 = _estimate_convol_params(
        precip_fields_lagr_diff[-2],
        precip_fields_lagr_diff[-1],
        convol_weights,
        mask_adv,
        kernel_type=kernel_type,
        num_workers=num_workers,
    )
    fct_gen["kernels_1"] = kernels_1

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    # compute convolved difference fields
    precip_fields_lagr_diff_c = precip_fields_lagr_diff[:-1].copy()
    for i in range(precip_fields_lagr_diff_c.shape[0]):
        for j in range(ari_order - i):
            precip_fields_lagr_diff_c[i] = _composite_convolution(
                precip_fields_lagr_diff_c[i],
                kernels_1,
                interp_weights,
            )

    print("Estimating the ARI(p,1) parameters... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    # estimate ARI(p,1) parameters
    weights = _compute_window_weights(
        feature_coords,
        precip_fields.shape[1],
        precip_fields.shape[2],
        ari_window_radius,
    )

    if ari_order == 1:
        psi = _estimate_ar1_params(
            precip_fields_lagr_diff_c[-1],
            precip_fields_lagr_diff[-1],
            weights,
            interp_weights,
            num_workers=num_workers,
        )
    else:
        psi = _estimate_ar2_params(
            precip_fields_lagr_diff_c[-2:],
            precip_fields_lagr_diff[-1],
            weights,
            interp_weights,
            num_workers=num_workers,
        )
    fct_gen["psi"] = psi

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    # apply the ARI(p,1) model and integrate the differences
    precip_fields_lagr_diff_c = _iterate_ar_model(precip_fields_lagr_diff_c, psi)
    precip_fct = precip_fields_lagr[-2] + precip_fields_lagr_diff_c[-1]
    precip_fct[precip_fct < 0.0] = 0.0

    print("Estimating the second convolution kernel... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    # estimate the second convolution kernels based on the forecast field
    # computed above
    kernels_2 = _estimate_convol_params(
        precip_fct,
        precip_fields[-1],
        convol_weights,
        mask_adv,
        kernel_type=kernel_type,
        num_workers=num_workers,
    )
    fct_gen["kernels_2"] = kernels_2

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    if measure_time:
        return fct_gen, precip_fields_lagr_diff, time.time() - starttime_init
    else:
        return fct_gen, precip_fields_lagr_diff


# Compute convolution where non-finite values are ignored.
def _masked_convolution(field, kernel):
    mask = np.isfinite(field)

    field = field.copy()
    field[~mask] = 0.0

    field_c = np.ones(field.shape) * np.nan
    field_c[mask] = convolve(field, kernel, mode="same")[mask]
    field_c[mask] /= convolve(mask.astype(float), kernel, mode="same")[mask]

    return field_c


# Compute standard deviation of forecast errors with spatially varying weights.
# Values close to zero are omitted.
def _weighted_std(f, w):
    mask = np.abs(f - 1.0) > 1e-4
    c = (w[mask].size - 1.0) / w[mask].size

    return np.sqrt(np.sum(w[mask] * (f[mask] - 1.0) ** 2.0) / (c * np.sum(w[mask])))


# Tukey window function centered at the given coordinates.
def _window_tukey(m, n, ci, cj, ri, rj, alpha=0.5):
    j, i = np.meshgrid(np.arange(n), np.arange(m))

    di = np.abs(i - ci)
    dj = np.abs(j - cj)

    mask1 = np.logical_and(di <= ri, dj <= rj)

    w1 = np.empty(di.shape)
    mask2 = di <= alpha * ri
    mask12 = np.logical_and(mask1, ~mask2)
    w1[mask12] = 0.5 * (
        1.0 + np.cos(np.pi * (di[mask12] - alpha * ri) / ((1.0 - alpha) * ri))
    )
    w1[np.logical_and(mask1, mask2)] = 1.0

    w2 = np.empty(dj.shape)
    mask2 = dj <= alpha * rj
    mask12 = np.logical_and(mask1, ~mask2)
    w2[mask12] = 0.5 * (
        1.0 + np.cos(np.pi * (dj[mask12] - alpha * rj) / ((1.0 - alpha) * rj))
    )
    w2[np.logical_and(mask1, mask2)] = 1.0

    weights = np.zeros((m, n))
    weights[mask1] = w1[mask1] * w2[mask1]

    return weights


# Uniform window function with all values set to one.
def _window_uniform(m, n, ci, cj, ri, rj):
    return np.ones((m, n))
