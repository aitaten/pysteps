"""
pysteps.nowcasts.linda
======================

This module implements the Lagrangian INtegro-Difference equation model with
Autoregression (LINDA). The model combines extrapolation, S-PROG, STEPS, ANVIL,
integro-difference equation (IDE) and cell tracking methods. It can produce
both deterministic and probabilistic nowcasts. LINDA is particularly designed
for nowcasting intense localized rainfall. For this purpose, it is expected to
give better forecast skill than S-PROG or STEPS.

The model consists of the following components:

1. feature detection to identify rain cells
2. advection-based extrapolation
3. autoregressive integrated ARI(p,1) process for growth and decay
4. convolution to account for loss of predictability
5. stochastic perturbations to simulate forecast errors

To allow localization to cells containing intense rainfall, LINDA utilizes a
sparse feature-based representation of the input data. Building on extrapolation
nowcast, the temporal evolution of rainfall fields is modeled in the Lagrangian
coordinates. Using the ARI process is adapted from ANVIL :cite:`PCLH2020`, and
the convolution is adapted from the integro-difference equation (IDE)
methodology proposed in :cite:`FW2005` and :cite:`XWF2005`. The combination of
these two approaches essentially replaces the cascade-based autoregressive
process used in S-PROG and STEPS. Using the convolution gives several
advantages such as the ability to handle anisotropic structure, domain
boundaries and missing data. Based on the marginal distribution and covariance
structure of forecast errors, localized perturbations are generated by adapting
the short-space Fourier transform (SSFT) methodology developed in
:cite:`NBSG2017`.

.. autosummary::
    :toctree: ../generated/

    forecast
"""

import time

try:
    import dask

    DASK_IMPORTED = True
except ImportError:
    DASK_IMPORTED = False
import numpy as np
from scipy.integrate import nquad
from scipy.interpolate import interp1d
from scipy.optimize import least_squares, LinearConstraint, minimize, minimize_scalar
from scipy.signal import convolve
from scipy import stats
from pysteps import extrapolation
from pysteps import feature


def forecast(
    precip_fields,
    advection_field,
    timesteps,
    feature_method="blob",
    feature_kwargs={},
    ari_order=1,
    kernel_type="anisotropic",
    interp_window_radius=None,
    convol_window_radius=None,
    ari_window_radius=None,
    extrap_method="semilagrangian",
    extrap_kwargs={},
    add_perturbations=True,
    num_ens_members=24,
    num_workers=1,
    measure_time=False,
):
    """Generate a nowcast ensemble by using the Lagrangian INtegro-Difference
    equation model with Autoregression (LINDA).

    Parameters
    ----------
    precip_fields : array_like
        Array of shape (ari_order + 2, m, n) containing the input rain rate
        or reflectivity fields (in linear scale) ordered by timestamp from
        oldest to newest. The time steps between the inputs are assumed to be
        regular.
    advection_field : array_like
        Array of shape (2, m, n) containing the x- and y-components of the
        advection field. The velocities are assumed to represent one time step
        between the inputs.
    timesteps : int or list of floats
        Number of time steps to forecast or a list of time steps for which the
        forecasts are computed (relative to the input time step). The elements
        of the list are required to be in ascending order.
    feature_method : {'blob', 'domain' 'grid', 'shitomasi'}
        Feature detection method:

        +-------------------+-----------------------------------------------------+
        |    Method name    |                  Description                        |
        +===================+=====================================================+
        |  blob             | Laplacian of Gaussian (LoG) blob detector           |
        |                   | implemented in scikit-image                         |
        +-------------------+-----------------------------------------------------+
        |  domain           | no feature detection, the model is applied over the |
        |                   | whole domain without localization                   |
        +-------------------+-----------------------------------------------------+
        |  grid             | no feature detection: the coordinates of the        |
        |                   | localization windows are aligned in a grid          |
        +-------------------+-----------------------------------------------------+
        |  shitomasi        | Shi-Tomasi corner detector implemented in OpenCV    |
        +-------------------+-----------------------------------------------------+

        Default : 'blob'
    feature_kwargs : dict, optional
        Keyword arguments that are passed as **kwargs for the feature detector.
        See :py:mod:`pysteps.feature.blob` and :py:mod:`pysteps.feature.shitomasi`.
    ari_order : {1, 2}
        The order of the ARI(p,1) model.
    kernel_type : {"anisotropic", "isotropic"}
        The type of the kernel. Default : 'anisotropic'.
    interp_window_radius : float
        The standard deviation of the Gaussian kernel for computing the
        interpolation weights. Default : 0.2 * min(m, n).
    convol_window_radius : float
        The standard deviation of the Gaussian window for estimating the
        convolution parameters. Default : 0.15 * min(m, n).
    ari_window_radius : float
        The standard deviation of the Gaussian window for estimating the
        ARI parameters. Default : 0.25 * min(m, n).
    extrap_method : str, optional
        The extrapolation method to use. See the documentation of
        :py:mod:`pysteps.extrapolation.interface`.
    extrap_kwargs : dict, optional
        Optional dictionary containing keyword arguments for the extrapolation
        method. See the documentation of :py:mod:`pysteps.extrapolation`.
    add_perturbations : bool
        Set to False to disable perturbations and generate a single
        deterministic nowcast.
    num_ens_members : int
        The number of ensemble members to generate.
    num_workers : int
        The number of workers to use for parallel computations. Applicable if
        dask is installed. When num_workers>1, it is advisable to disable
        OpenMP by setting the environment variable OMP_NUM_THREADS to 1. This
        avoids slowdown caused by too many simultaneous threads.
    measure_time: bool
        If set to True, measure, print and return the computation time.

    Returns
    -------
    out : numpy.ndarray
        A four-dimensional array of shape (num_ens_members, num_timesteps, m, n)
        containing a time series of forecast precipitation fields for each
        ensemble member. The time series starts from t0 + timestep, where
        timestep is taken from the input fields.
    """
    _check_inputs(precip_fields, advection_field, timesteps, ari_order)

    precip_fields = precip_fields[-(ari_order + 2) :]
    input_length = precip_fields.shape[0]

    if interp_window_radius is None:
        interp_window_radius = 0.2 * np.min(precip_fields.shape[1:])
    if convol_window_radius is None:
        convol_window_radius = 0.15 * np.min(precip_fields.shape[1:])
    if ari_window_radius is None:
        ari_window_radius = 0.25 * np.min(precip_fields.shape[1:])

    print("Computing LINDA nowcast:")
    print("------------------------")
    print("")

    print("Inputs:")
    print("-------")
    print(
        "input dimensions: {}x{}".format(precip_fields.shape[1], precip_fields.shape[2])
    )
    print("")

    print("Methods:")
    print("--------")
    if add_perturbations:
        print("nowcast type:     ensemble")
    else:
        print("nowcast type:     deterministic")
    print("feature detector: {}".format(feature_method))
    print("extrapolator:     {}".format(extrap_method))
    print("")

    print("Parameters:")
    print("-----------")
    if isinstance(timesteps, int):
        print("number of time steps:  {}".format(timesteps))
    else:
        # TODO: implement fractional time steps
        raise NotImplementedError("fractional time steps not yet implemented")
        print("time steps:            {}".format(timesteps))
    print("ARI model order:       {}".format(ari_order))
    print("convol. window radius: {}".format(convol_window_radius))
    print("ARI window radius:     {}".format(ari_window_radius))
    print("interp. window radius: {}".format(interp_window_radius))
    if add_perturbations:
        print("ensemble size:         {}".format(num_ens_members))
    print("parallel workers:      {}".format(num_workers))

    starttime_init = time.time()

    extrapolator = extrapolation.get_method(extrap_method)
    extrap_kwargs = extrap_kwargs.copy()
    extrap_kwargs["allow_nonfinite_values"] = True

    # detect features from the most recent input field
    # TODO: implement the "grid" option
    if feature_method in {"blob", "shitomasi"}:
        precip_field_ = precip_fields[-1].copy()
        precip_field_[~np.isfinite(precip_field_)] = 0.0
        feature_detector = feature.get_method(feature_method)

        if measure_time:
            starttime = time.time()

        feature_coords = np.fliplr(
            feature_detector(precip_field_, **feature_kwargs)[:, :2]
        )

        feature_type = "blobs" if feature_method == "blob" else "corners"
        print("")
        print("Detecting features... ", end="", flush=True)
        if measure_time:
            print(
                "found {} {} in {:.2f} seconds.".format(
                    feature_coords.shape[0], feature_type, time.time() - starttime
                )
            )
        else:
            print("found {} {}.".format(feature_coords.shape[0], feature_type))
    elif feature_method == "domain":
        feature_coords = np.zeros((1, 2), dtype=int)
    else:
        raise NotImplementedError(
            "feature detector '%s' not implemented" % feature_method
        )

    # compute interpolation weights
    interp_weights = _compute_window_weights(
        feature_coords,
        precip_fields.shape[1],
        precip_fields.shape[2],
        interp_window_radius,
    )
    interp_weights /= np.sum(interp_weights, axis=0)

    # transform the input fields to the Lagrangian coordinates
    precip_fields_lagr = np.empty(precip_fields.shape)

    def worker(i):
        precip_fields_lagr[i, :] = extrapolator(
            precip_fields[i, :],
            advection_field,
            input_length - 1 - i,
            "min",
            **extrap_kwargs,
        )[-1]

    if DASK_IMPORTED and num_workers > 1:
        res = []

    print("Transforming to Lagrangian coordinates... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    for i in range(precip_fields.shape[0] - 1):
        if DASK_IMPORTED and num_workers > 1:
            res.append(dask.delayed(worker)(i))
        else:
            worker(i)

    if DASK_IMPORTED and num_workers > 1:
        dask.compute(*res, num_workers=min(num_workers, len(res)), scheduler="threads")
    precip_fields_lagr[-1] = precip_fields[-1]

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    # compute advection mask and set nan to pixels, where one or more of the
    # advected input fields has a nan value
    mask_adv = np.all(np.isfinite(precip_fields_lagr), axis=0)
    for i in range(precip_fields_lagr.shape[0]):
        precip_fields_lagr[i, ~mask_adv] = np.nan

    # compute differenced input fields in the Lagrangian coordinates
    precip_fields_lagr_diff = np.diff(precip_fields_lagr, axis=0)

    # estimate parameters of the deterministic model (i.e. the convolution and
    # the ARI process)

    print("Estimating the first convolution kernel... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    # estimate convolution kernel for the differenced component
    convol_weights = _compute_window_weights(
        feature_coords,
        precip_fields.shape[1],
        precip_fields.shape[2],
        convol_window_radius,
    )

    kernels_1 = _estimate_convol_params(
        precip_fields_lagr_diff[-2],
        precip_fields_lagr_diff[-1],
        convol_weights,
        mask_adv,
        kernel_type=kernel_type,
        num_workers=num_workers,
    )

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    # compute convolved difference fields
    precip_fields_lagr_diff_c = precip_fields_lagr_diff[:-1].copy()
    for i in range(precip_fields_lagr_diff_c.shape[0]):
        for j in range(ari_order - i):
            precip_fields_lagr_diff_c[i] = _composite_convolution(
                precip_fields_lagr_diff_c[i],
                kernels_1,
                interp_weights,
            )

    print("Estimating the ARI(p,1) parameters... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    # estimate ARI(p,1) parameters
    weights = _compute_window_weights(
        feature_coords,
        precip_fields.shape[1],
        precip_fields.shape[2],
        ari_window_radius,
    )

    if ari_order == 1:
        psi = _estimate_ar1_params(
            precip_fields_lagr_diff_c[-1],
            precip_fields_lagr_diff[-1],
            weights,
            interp_weights,
            num_workers=num_workers,
        )
    else:
        psi = _estimate_ar2_params(
            precip_fields_lagr_diff_c[-2:],
            precip_fields_lagr_diff[-1],
            weights,
            interp_weights,
            num_workers=num_workers,
        )

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    # apply the ARI(p,1) model and integrate the differences
    precip_fields_lagr_diff_c = _iterate_ar_model(precip_fields_lagr_diff_c, psi)
    precip_fct = precip_fields_lagr[-2] + precip_fields_lagr_diff_c[-1]
    precip_fct[precip_fct < 0.0] = 0.0

    print("Estimating the second convolution kernel... ", end="", flush=True)

    if measure_time:
        starttime = time.time()

    # estimate the second convolution kernels based on the forecast field
    # computed above
    kernels_2 = _estimate_convol_params(
        precip_fct,
        precip_fields[-1],
        convol_weights,
        mask_adv,
        kernel_type=kernel_type,
        num_workers=num_workers,
    )

    if measure_time:
        print("{:.2f} seconds.".format(time.time() - starttime))
    else:
        print("done.")

    # compute the nowcast
    precip_fct = precip_fields[-1].copy()
    precip_fields_lagr_diff = precip_fields_lagr_diff[1:].copy()

    displacement = None
    extrap_kwargs["return_displacement"] = True
    precip_out = []

    for i in range(precip_fields_lagr_diff.shape[0]):
        for j in range(ari_order - i):
            precip_fields_lagr_diff[i] = _composite_convolution(
                precip_fields_lagr_diff[i],
                kernels_1,
                interp_weights,
            )

    if measure_time:
        init_time = time.time() - starttime_init

    # iterate each time step
    if measure_time:
        starttime_mainloop = time.time()

    # TODO: Implement using a list instead of a number of fixed timesteps
    for t in range(timesteps):
        print(
            "Computing nowcast for time step %d... " % (t + 1),
            end="",
            flush=True,
        )

        if measure_time:
            starttime = time.time()

        precip_fields_lagr_diff = _iterate_ar_model(precip_fields_lagr_diff, psi)
        precip_fct += precip_fields_lagr_diff[-1]
        for i in range(precip_fields_lagr_diff.shape[0]):
            precip_fields_lagr_diff[i] = _composite_convolution(
                precip_fields_lagr_diff[i],
                kernels_1,
                interp_weights,
            )
        precip_fct = _composite_convolution(precip_fct, kernels_2, interp_weights)

        precip_out_ = precip_fct.copy()
        precip_out_[precip_out_ < 0.0] = 0.0
        precip_out_[~mask_adv] = np.nan

        # advect the forecast field for t time steps
        extrap_kwargs["displacement_prev"] = displacement
        precip_out_, displacement = extrapolator(
            precip_out_, advection_field, 1, **extrap_kwargs
        )
        precip_out.append(precip_out_[0])

        if measure_time:
            print("{:.2f} seconds.".format(time.time() - starttime))
        else:
            print("done.")

    if measure_time:
        mainloop_time = time.time() - starttime_mainloop
        return np.stack(precip_out), init_time, mainloop_time
    else:
        return np.stack(precip_out)


def _check_inputs(precip_fields, advection_field, timesteps, ari_order):
    if ari_order not in [1, 2]:
        raise ValueError(f"ari_order {ari_order} given, 1 or 2 required")
    if len(precip_fields.shape) != 3:
        raise ValueError("precip_fields must be a three-dimensional array")
    if precip_fields.shape[0] < ari_order + 2:
        raise ValueError("precip_fields.shape[0] < ari_order+2")
    if len(advection_field.shape) != 3:
        raise ValueError("advection_field must be a three-dimensional array")
    if precip_fields.shape[1:3] != advection_field.shape[1:3]:
        raise ValueError(
            "dimension mismatch between precip_fields and advection_field: precip_fields.shape={}, advection_field.shape={}".format(
                precip_fields.shape, advection_field.shape
            )
        )
    if isinstance(timesteps, list) and not sorted(timesteps) == timesteps:
        raise ValueError("timesteps is not in ascending order")


# Compute a localized convolution by applying a set of kernels with the given
# spatial weights. The weights are assumed to be normalized.
def _composite_convolution(field, kernels, weights):
    n = len(kernels)
    field_c = 0.0

    for i in range(n):
        field_c += weights[i] * _masked_convolution(field, kernels[i])

    return field_c


# compute the inverse ACF mapping between two distributions
def _compute_inverse_acf_mapping(target_dist, target_dist_params, n_intervals=20):
    phi = (
        lambda x1, x2, rho: 1.0
        / (2 * np.pi * np.sqrt(1 - rho ** 2))
        * np.exp(-(x1 ** 2 + x2 ** 2 - 2 * rho * x1 * x2) / (2 * (1 - rho ** 2)))
    )

    rho_1 = np.linspace(-0.999, 0.999, n_intervals)
    rho_2 = np.empty(len(rho_1))

    mu = target_dist.mean(*target_dist_params)
    sigma = target_dist.std(*target_dist_params)

    cdf_trans = lambda x: target_dist.ppf(stats.norm.cdf(x), *target_dist_params)
    int_range = (-6, 6)

    for i, rho_1_ in enumerate(rho_1):
        f = (
            lambda x1, x2: (cdf_trans(x1) - mu)
            * (cdf_trans(x2) - mu)
            * phi(x1, x2, rho_1_)
        )
        opts = {"epsabs": 1e-4, "epsrel": 1e-4, "limit": 1}
        rho_2[i] = nquad(f, (int_range, int_range), opts=opts)[0] / (sigma * sigma)

    return interp1d(np.hstack([-1.0, rho_2, 1.0]), np.hstack([-1.0, rho_1, 1.0]))


# Compute anisotropic Gaussian convolution kernel
def _compute_kernel_anisotropic(params, cutoff=6.0):
    phi, sigma1, sigma2, alpha = params

    sigma1 = abs(sigma1)
    sigma2 = abs(sigma2)

    phi_r = phi / 180.0 * np.pi
    R_inv = np.array([[np.cos(phi_r), np.sin(phi_r)], [-np.sin(phi_r), np.cos(phi_r)]])

    bb_y1, bb_x1, bb_y2, bb_x2 = _compute_ellipse_bbox(phi, sigma1, sigma2, cutoff)

    x = np.arange(int(bb_x1), int(bb_x2) + 1).astype(float)
    if len(x) % 2 == 0:
        x = np.arange(int(bb_x1) - 1, int(bb_x2) + 1).astype(float)
    y = np.arange(int(bb_y1), int(bb_y2) + 1).astype(float)
    if len(y) % 2 == 0:
        y = np.arange(int(bb_y1) - 1, int(bb_y2) + 1).astype(float)

    X, Y = np.meshgrid(x, y)
    XY = np.vstack([X.flatten(), Y.flatten()])
    XY = np.dot(R_inv, XY)

    x2 = XY[0, :] * XY[0, :]
    y2 = XY[1, :] * XY[1, :]
    result = np.exp(-((x2 / sigma1 + y2 / sigma2) ** alpha))
    result /= np.sum(result)

    return np.reshape(result, X.shape)


def _compute_kernel_isotropic(sigma, cutoff=6.0):
    bb_y1, bb_x1, bb_y2, bb_x2 = (
        -sigma * cutoff,
        -sigma * cutoff,
        sigma * cutoff,
        sigma * cutoff,
    )

    x = np.arange(int(bb_x1), int(bb_x2) + 1).astype(float)
    if len(x) % 2 == 0:
        x = np.arange(int(bb_x1) - 1, int(bb_x2) + 1).astype(float)
    y = np.arange(int(bb_y1), int(bb_y2) + 1).astype(float)
    if len(y) % 2 == 0:
        y = np.arange(int(bb_y1) - 1, int(bb_y2) + 1).astype(float)

    X, Y = np.meshgrid(x / sigma, y / sigma)

    r2 = X * X + Y * Y
    result = np.exp(-0.5 * r2)

    return result / np.sum(result)


# Compute the bounding box of an ellipse
def _compute_ellipse_bbox(phi, sigma1, sigma2, cutoff):
    r1 = cutoff * sigma1
    r2 = cutoff * sigma2
    phi_r = phi / 180.0 * np.pi

    if np.abs(phi_r - np.pi / 2) > 1e-6 and np.abs(phi_r - 3 * np.pi / 2) > 1e-6:
        alpha = np.arctan(-r2 * np.sin(phi_r) / (r1 * np.cos(phi_r)))
        w = r1 * np.cos(alpha) * np.cos(phi_r) - r2 * np.sin(alpha) * np.sin(phi_r)

        alpha = np.arctan(r2 * np.cos(phi_r) / (r1 * np.sin(phi_r)))
        h = r1 * np.cos(alpha) * np.sin(phi_r) + r2 * np.sin(alpha) * np.cos(phi_r)
    else:
        w = sigma2 * cutoff
        h = sigma1 * cutoff

    return -abs(h), -abs(w), abs(h), abs(w)


def _compute_window_weights(coords, grid_height, grid_width, window_radius):
    coords = coords.astype(float).copy()
    num_features = coords.shape[0]

    coords[:, 0] /= grid_height
    coords[:, 1] /= grid_width

    window_radius_1 = window_radius / grid_height
    window_radius_2 = window_radius / grid_width

    grid_x = (np.arange(grid_width) + 0.5) / grid_width
    grid_y = (np.arange(grid_height) + 0.5) / grid_height

    grid_x, grid_y = np.meshgrid(grid_x, grid_y)

    w = np.empty((num_features, grid_x.shape[0], grid_x.shape[1]))

    if coords.shape[0] > 1:
        for i, c in enumerate(coords):
            dy = c[0] - grid_y
            dx = c[1] - grid_x

            w[i, :] = np.exp(
                -dy * dy / (2 * window_radius_1 ** 2)
                - dx * dx / (2 * window_radius_2 ** 2)
            )
    else:
        w[0, :] = np.ones((grid_height, grid_width))

    return w


# compute a parametric ACF
def _compute_acf(params, m, n, func):
    c, phi, sigma1, sigma2 = params

    sigma1 = abs(sigma1)
    sigma2 = abs(sigma2)

    phi_r = phi / 180.0 * np.pi
    rot_inv = np.array(
        [[np.cos(phi_r), np.sin(phi_r)], [-np.sin(phi_r), np.cos(phi_r)]]
    )

    if m % 2 != 0 or n % 2 != 0:
        raise ValueError("m and n must be even")

    x = np.fft.ifftshift(np.arange(-int(n / 2), int(n / 2)))
    y = np.fft.ifftshift(np.arange(-int(m / 2), int(m / 2)))

    grid_x, grid_y = np.meshgrid(x, y)
    grid_xy = np.vstack([grid_x.flatten(), grid_y.flatten()])
    grid_xy = np.dot(rot_inv, grid_xy)

    grid_xy[0, :] = grid_xy[0, :] / sigma1
    grid_xy[1, :] = grid_xy[1, :] / sigma2

    r2 = np.reshape(
        grid_xy[0, :] * grid_xy[0, :] + grid_xy[1, :] * grid_xy[1, :], grid_x.shape
    )
    if func == "gauss":
        result = np.exp(-0.5 * r2)
    else:
        r2 = np.sqrt(r2)
        result = np.exp(-r2)

    return c * result


# Constrained optimization of AR(1) parameters.
def _estimate_ar1_params(
    field_src, field_dst, estim_weights, interp_weights, num_workers=1
):
    def objf(p, *args):
        i = args[0]
        field_ar = p * field_src
        return np.nansum(estim_weights[i] * (field_dst - field_ar) ** 2.0)

    bounds = (-0.98, 0.98)

    def worker(i):
        return minimize_scalar(objf, method="bounded", bounds=bounds, args=(i,)).x

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(len(estim_weights)):
            res.append(dask.delayed(worker)(i))

        psi = dask.compute(*res, num_workers=num_workers, scheduler="threads")
    else:
        psi = []
        for i in range(len(estim_weights)):
            psi.append(worker(i))

    return [np.sum([psi[i] * interp_weights[i] for i in range(len(psi))], axis=0)]


# Constrained optimization of AR(2) parameters.
def _estimate_ar2_params(
    field_src, field_dst, estim_weights, interp_weights, num_workers=1
):
    def objf(p, *args):
        i = args[0]
        field_ar = p[0] * field_src[1] + p[1] * field_src[0]
        return np.nansum(estim_weights[i] * (field_dst - field_ar) ** 2.0)

    bounds = [(-1.98, 1.98), (-0.98, 0.98)]
    constraints = [
        LinearConstraint(
            np.array([(1, 1), (-1, 1)]),
            (-np.inf, -np.inf),
            (0.98, 0.98),
            keep_feasible=True,
        )
    ]

    def worker(i):
        return minimize(
            objf,
            (0.8, 0.0),
            method="trust-constr",
            bounds=bounds,
            constraints=constraints,
            args=(i,),
        ).x

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(len(estim_weights)):
            res.append(dask.delayed(worker)(i))

        psi = dask.compute(*res, num_workers=num_workers, scheduler="threads")
    else:
        psi = []
        for i in range(len(estim_weights)):
            psi.append(worker(i))

    psi_out = []
    for i in range(2):
        psi_out.append(
            np.sum([psi[j][i] * interp_weights[j] for j in range(len(psi))], axis=0)
        )

    return psi_out


def _estimate_convol_params(
    field_src,
    field_dst,
    weights,
    mask,
    kernel_type="anisotropic",
    kernel_params={},
    num_workers=1,
):
    masks = []
    for i in range(len(weights)):
        masks.append(np.logical_and(mask, weights[i] > 1e-3))

    def objf_aniso(p, *args):
        i = args[0]
        p = _get_anisotropic_kernel_params(p)
        kernel = _compute_kernel_anisotropic(p, **kernel_params)

        field_src_c = _masked_convolution(field_src, kernel)
        fval = np.sqrt(weights[i][masks[i]]) * (
            field_dst[masks[i]] - field_src_c[masks[i]]
        )

        return fval

    def objf_iso(p, *args):
        i = args[0]
        kernel = _compute_kernel_isotropic(p, **kernel_params)

        field_src_c = _masked_convolution(field_src, kernel)
        fval = np.sum(
            weights[i][masks[i]] * (field_dst[masks[i]] - field_src_c[masks[i]]) ** 2
        )

        return fval

    def worker(i):
        if kernel_type == "anisotropic":
            bounds = np.array([(-10.0, -10.0, 0.25, 0.1), (10.0, 10.0, 4.0, 5.0)])
            p_opt = least_squares(
                objf_aniso,
                np.array((1.0, 1.0, 1.0, 1.0)),
                bounds=bounds,
                method="trf",
                ftol=1e-6,
                xtol=1e-6,
                gtol=1e-6,
                args=(i,),
            )
            p_opt = _get_anisotropic_kernel_params(p_opt.x)

            return _compute_kernel_anisotropic(p_opt, **kernel_params)
        else:
            p_opt = minimize_scalar(
                objf_iso, bounds=[0.01, 10.0], method="bounded", args=(i,)
            )
            p_opt = p_opt.x

            return _compute_kernel_isotropic(p_opt, **kernel_params)

    if DASK_IMPORTED and num_workers > 1:
        res = []
        for i in range(len(weights)):
            res.append(dask.delayed(worker)(i))
        kernels = dask.compute(*res, num_workers=num_workers, scheduler="threads")
    else:
        kernels = []
        for i in range(len(weights)):
            kernels.append(worker(i))

    return kernels


# fit a parametric ACF to the given sample estimate
def _fit_acf(acf, method="trf"):
    def objf(p, *args):
        p = _get_anisotropic_kernel_params(p)
        fitted_acf = _compute_acf(p, acf.shape[0], acf.shape[1], "exp")

        return (acf - fitted_acf).flatten()

    bounds = np.array([(0.01, -10.0, -10.0, 0.1), (10.0, 10.0, 10.0, 10.0)])
    p_opt = least_squares(
        objf,
        np.array((1.0, 1.0, 1.0, 1.0)),
        bounds=bounds,
        method="trf",
        ftol=1e-6,
        xtol=1e-6,
        gtol=1e-6,
    )

    return _compute_acf(
        _get_anisotropic_kernel_params(p_opt.x), acf.shape[0], acf.shape[1], "exp"
    )


# fit a lognormal distribution by maximizing the log-likelihood function with
# the constraint that the mean value is one
def _fit_dist(err, dist, wf, mask):
    f = lambda p: -np.sum(np.log(stats.lognorm.pdf(err[mask], p, -0.5 * p ** 2)))
    p_opt = minimize_scalar(f, bounds=(1e-3, 20.0), method="Bounded")

    return (p_opt.x, -0.5 * p_opt.x ** 2)


# Get anisotropic convolution kernel parameters from the given parameter vector.
def _get_anisotropic_kernel_params(p):
    theta = np.arctan2(p[1], p[0])
    sigma1 = np.sqrt(p[0] * p[0] + p[1] * p[1])
    sigma2 = sigma1 * p[2]

    return theta, sigma1, sigma2, p[3]


# TODO: use the method implemented in pysteps.timeseries.autoregression
def _iterate_ar_model(input_fields, psi):
    input_field_new = 0.0

    for i in range(len(psi)):
        input_field_new += psi[i] * input_fields[-(i + 1), :]

    return np.concatenate([input_fields[1:, :], input_field_new[np.newaxis, :]])


# Compute convolution where non-finite values are ignored.
def _masked_convolution(field, kernel):
    mask = np.isfinite(field)

    field = field.copy()
    field[~mask] = 0.0

    field_c = np.ones(field.shape) * np.nan
    field_c[mask] = convolve(field, kernel, mode="same")[mask]
    field_c[mask] /= convolve(mask.astype(float), kernel, mode="same")[mask]

    return field_c
